{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0a0f1-6815-45ca-a335-481db718aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 6: Adversarial Machine Learning for Cybersecurity\n",
    "# Student Interactive Jupyter Notebook\n",
    "\n",
    "# CELL 1: Introduction and Setup\n",
    "\"\"\"\n",
    "Course: AI in Cybersecurity\n",
    "Lab Duration: 60-75 minutes\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Run each cell in order (Shift+Enter)\n",
    "2. Read the markdown cells between code cells for guidance\n",
    "3. Fill in observations when prompted\n",
    "4. Don't skip sections - each builds on the previous\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "EPSILON = 0.1  # Attack strength\n",
    "EPOCHS_STANDARD = 10  # Training epochs for standard model\n",
    "EPOCHS_ROBUST = 5     # Training epochs for robust model\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(\"âœ“ Random seeds set for reproducible results\")\n",
    "print(f\"âœ“ Configuration set: Îµ={EPSILON}, Standard epochs={EPOCHS_STANDARD}, Robust epochs={EPOCHS_ROBUST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67d10f-fb6b-46c3-9771-8269eaa2d5f0",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Understanding the Threat\n",
    "\n",
    "### What are adversarial attacks?\n",
    "Adversarial attacks are small, carefully crafted modifications to input data that cause \n",
    "machine learning models to make incorrect predictions while appearing normal to humans.\n",
    "\n",
    "### Why do they matter in cybersecurity?\n",
    "- Malware can be modified to evade detection systems\n",
    "- Facial recognition can be fooled with special patterns  \n",
    "- Network intrusion detection can be bypassed\n",
    "- Any ML-based security system is potentially vulnerable\n",
    "\n",
    "### Your Task:\n",
    "Write your hypothesis about what makes ML models vulnerable:\n",
    "\n",
    "**HYPOTHESIS:** _______________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd3758-fd1a-4811-a9dc-a328e71a8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Demo Class Definition\n",
    "class AdversarialMLDemo:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.robust_model = None\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "        self.class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "        self.training_history = None\n",
    "        self.robust_training_history = None\n",
    "    \n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load and preprocess CIFAR-10 dataset\"\"\"\n",
    "        print(\"Loading CIFAR-10 dataset...\")\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "        \n",
    "        # Normalize pixel values to [0, 1]\n",
    "        self.x_train = x_train.astype('float32') / 255.0\n",
    "        self.x_test = x_test.astype('float32') / 255.0\n",
    "        \n",
    "        # Convert labels to categorical\n",
    "        self.y_train = to_categorical(y_train, 10)\n",
    "        self.y_test = to_categorical(y_test, 10)\n",
    "        \n",
    "        print(f\"âœ“ Training data shape: {self.x_train.shape}\")\n",
    "        print(f\"âœ“ Test data shape: {self.x_test.shape}\")\n",
    "        \n",
    "        # Show sample images\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "        fig.suptitle('Sample CIFAR-10 Images', fontsize=16)\n",
    "        for i in range(10):\n",
    "            row, col = i // 5, i % 5\n",
    "            axes[row, col].imshow(self.x_train[i])\n",
    "            axes[row, col].set_title(f'{self.class_names[np.argmax(self.y_train[i])]}')\n",
    "            axes[row, col].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_simple_cnn(self):\n",
    "        \"\"\"Create a simple CNN model\"\"\"\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def train_model(self, epochs=EPOCHS_STANDARD):\n",
    "        \"\"\"Train the standard (vulnerable) model\"\"\"\n",
    "        print(\"ðŸ—ï¸ Training standard model...\")\n",
    "        print(\"This represents any ML model used in cybersecurity!\")\n",
    "        \n",
    "        self.model = self.create_simple_cnn()\n",
    "        \n",
    "        history = self.model.fit(self.x_train, self.y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(self.x_test, self.y_test),\n",
    "                                verbose=1)\n",
    "        \n",
    "        self.training_history = history.history\n",
    "        \n",
    "        # Evaluate model and show results clearly\n",
    "        test_loss, test_acc = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š TRAINING COMPLETE!\")\n",
    "        print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.3f}\")\n",
    "        print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.3f}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "        \n",
    "        return test_acc  # Return useful info instead of history object\n",
    "\n",
    "# Initialize demo\n",
    "demo = AdversarialMLDemo()\n",
    "\n",
    "# CELL 3: Load Data\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2: LOADING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "demo.load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c3e9cb-1517-401a-bfee-cc7d48a45ec2",
   "metadata": {},
   "source": [
    "## Part 2: Building a Vulnerable Model\n",
    "\n",
    "Now we'll create a \"normal\" CNN that we can attack. This represents any \n",
    "ML model used in cybersecurity (malware detector, intrusion detection, etc.).\n",
    "\n",
    "**Observation:** Look at the sample images above. These are the 10 classes our model will learn to recognize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d1423-ac10-4296-811a-f612c997dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Train Model\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2: TRAINING VULNERABLE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the model (this will take a few minutes)\n",
    "final_accuracy = demo.train_model(epochs=EPOCHS_STANDARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2940c-cf14-4652-a1a0-234c2d3c178b",
   "metadata": {},
   "source": [
    "## ðŸ¤” OBSERVATION CHECKPOINT - Part 2\n",
    "\n",
    "Record your results:\n",
    "- **Final Training Accuracy:** _______%\n",
    "- **Final Test Accuracy:** _______%\n",
    "- **Training time:** _______ minutes\n",
    "\n",
    "This model will now be our \"victim\" - let's see how easily we can fool it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ed233-ea2c-4a2a-bb21-c1c4b9095663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: FGSM Attack Function and Quick Demo\n",
    "def fgsm_attack(model, image, label, epsilon=EPSILON):\n",
    "    \"\"\"Fast Gradient Sign Method attack\"\"\"\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(label, prediction)\n",
    "    \n",
    "    # Get the gradients - this shows model's weaknesses!\n",
    "    gradient = tape.gradient(loss, image)\n",
    "    \n",
    "    # Create the attack by following the gradient\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    adversarial_image = image + epsilon * signed_grad\n",
    "    adversarial_image = tf.clip_by_value(adversarial_image, 0, 1)\n",
    "    \n",
    "    return adversarial_image\n",
    "\n",
    "print(\"âœ“ FGSM attack function defined\")\n",
    "\n",
    "# Quick demonstration with one image\n",
    "print(\"ðŸŽ¯ Testing the attack function with one image...\")\n",
    "\n",
    "# Get a random test image\n",
    "test_idx = np.random.randint(0, len(demo.x_test))\n",
    "test_image = demo.x_test[test_idx:test_idx+1]\n",
    "test_label = demo.y_test[test_idx:test_idx+1]\n",
    "\n",
    "# Get original prediction\n",
    "original_pred = demo.model.predict(test_image, verbose=0)\n",
    "original_class = np.argmax(original_pred)\n",
    "\n",
    "# Generate adversarial example\n",
    "adversarial_image = fgsm_attack(demo.model, test_image, test_label, EPSILON)\n",
    "\n",
    "# Get adversarial prediction\n",
    "adv_pred = demo.model.predict(adversarial_image, verbose=0)\n",
    "adv_class = np.argmax(adv_pred)\n",
    "\n",
    "# Show the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.imshow(test_image[0])\n",
    "ax1.set_title(f'Original\\nPrediction: {demo.class_names[original_class]}\\nConfidence: {original_pred[0][original_class]:.3f}')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(adversarial_image[0])\n",
    "ax2.set_title(f'Adversarial\\nPrediction: {demo.class_names[adv_class]}\\nConfidence: {adv_pred[0][adv_class]:.3f}')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.suptitle('First Adversarial Attack Success!', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "attack_success = (original_class != adv_class)\n",
    "print(f\"Attack successful: {attack_success}\")\n",
    "print(f\"Changed prediction from '{demo.class_names[original_class]}' to '{demo.class_names[adv_class]}'\")\n",
    "print(\"Now let's see more examples...\")\n",
    "\n",
    "# CELL 6: Generate First Adversarial Examples\n",
    "def generate_and_show_attacks(demo, num_examples=5, epsilon=EPSILON):\n",
    "    \"\"\"Generate and visualize adversarial examples\"\"\"\n",
    "    print(f\"ðŸŽ¯ Generating {num_examples} adversarial examples with epsilon={epsilon}...\")\n",
    "    \n",
    "    # Select random test examples\n",
    "    indices = np.random.choice(len(demo.x_test), num_examples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_examples, figsize=(15, 6))\n",
    "    fig.suptitle('Your First Adversarial Attack Results! ðŸš¨', fontsize=16)\n",
    "    \n",
    "    attack_results = []\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get original image and label\n",
    "        original_img = demo.x_test[idx:idx+1]\n",
    "        true_label = demo.y_test[idx:idx+1]\n",
    "        true_class = np.argmax(true_label)\n",
    "        \n",
    "        # Generate adversarial example\n",
    "        adv_img = fgsm_attack(demo.model, original_img, true_label, epsilon)\n",
    "        \n",
    "        # Get predictions\n",
    "        orig_pred = demo.model.predict(original_img, verbose=0)\n",
    "        adv_pred = demo.model.predict(adv_img, verbose=0)\n",
    "        \n",
    "        orig_pred_class = np.argmax(orig_pred)\n",
    "        adv_pred_class = np.argmax(adv_pred)\n",
    "        \n",
    "        # Check if attack was successful\n",
    "        attack_success = (orig_pred_class == true_class and adv_pred_class != true_class)\n",
    "        attack_results.append({\n",
    "            'original': demo.class_names[orig_pred_class],\n",
    "            'adversarial': demo.class_names[adv_pred_class], \n",
    "            'true': demo.class_names[true_class],\n",
    "            'success': attack_success\n",
    "        })\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, i].imshow(original_img[0])\n",
    "        axes[0, i].set_title(f'Original\\nTrue: {demo.class_names[true_class]}\\nPred: {demo.class_names[orig_pred_class]}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Adversarial image  \n",
    "        axes[1, i].imshow(adv_img[0])\n",
    "        success_marker = \"âœ“\" if attack_success else \"âœ—\"\n",
    "        axes[1, i].set_title(f'Adversarial {success_marker}\\nTrue: {demo.class_names[true_class]}\\nPred: {demo.class_names[adv_pred_class]}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Highlight successful attacks with red border\n",
    "        if attack_success:\n",
    "            from matplotlib.patches import Rectangle\n",
    "            for ax in [axes[0, i], axes[1, i]]:\n",
    "                ax.add_patch(Rectangle((0, 0), 32, 32, fill=False, edgecolor='red', linewidth=2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show statistics\n",
    "    success_count = sum(r['success'] for r in attack_results)\n",
    "    success_rate = success_count / len(attack_results) * 100\n",
    "    print(f\"\\nðŸŽ¯ Attack Success Rate: {success_rate:.1f}%\")\n",
    "    print(f\"ðŸ’€ Successfully fooled the model {success_count} out of {len(attack_results)} times!\")\n",
    "    \n",
    "    return attack_results\n",
    "\n",
    "# Run the attack!\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 3: YOUR FIRST ADVERSARIAL ATTACK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "attack_results = generate_and_show_attacks(demo, num_examples=5, epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b282a-f833-4041-9614-cae89989fb8d",
   "metadata": {},
   "source": [
    "## ðŸ¤” OBSERVATION CHECKPOINT - Part 3\n",
    "\n",
    "Fill in your attack results:\n",
    "\n",
    "| Image | Original Prediction | Adversarial Prediction | Successful Attack? |\n",
    "|-------|--------------------|-----------------------|-------------------|\n",
    "| 1.    |                    |                       |                   |\n",
    "| 2.    |                    |                       |                   |\n",
    "| 3.    |                    |                       |                   |\n",
    "| 4.    |                    |                       |                   |\n",
    "| 5.    |                    |                       |                   |\n",
    "\n",
    "**CRITICAL QUESTION:** Can you visually tell the difference between original and adversarial images?\n",
    "\n",
    "**YOUR ANSWER:** ___________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d745ee-42d3-4115-9a25-92ca8348a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Visualize Attack Mechanics\n",
    "def visualize_attack_mechanics(demo, num_examples=3, epsilon=EPSILON):\n",
    "    \"\"\"Show the perturbations and how attacks work\"\"\"\n",
    "    print(\"ðŸ”¬ Analyzing attack mechanics...\")\n",
    "    \n",
    "    indices = np.random.choice(len(demo.x_test), num_examples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_examples, figsize=(12, 9))\n",
    "    fig.suptitle(f'How FGSM Attacks Work (Îµ={epsilon})', fontsize=16)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        original_img = demo.x_test[idx:idx+1]\n",
    "        true_label = demo.y_test[idx:idx+1]\n",
    "        \n",
    "        # Generate adversarial example and get perturbation\n",
    "        adv_img = fgsm_attack(demo.model, original_img, true_label, epsilon)\n",
    "        perturbation = adv_img[0] - original_img[0]\n",
    "        \n",
    "        # Get predictions\n",
    "        orig_pred = demo.model.predict(original_img, verbose=0)\n",
    "        adv_pred = demo.model.predict(adv_img, verbose=0)\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, i].imshow(original_img[0])\n",
    "        axes[0, i].set_title(f'Original\\n{demo.class_names[np.argmax(true_label)]}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Perturbation (amplified for visibility)\n",
    "        pert_vis = perturbation * 10 + 0.5  # Amplify and center\n",
    "        pert_vis = np.clip(pert_vis, 0, 1)\n",
    "        axes[1, i].imshow(pert_vis)\n",
    "        axes[1, i].set_title('Perturbation\\n(10x amplified)')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Adversarial result\n",
    "        axes[2, i].imshow(adv_img[0])\n",
    "        axes[2, i].set_title(f'Result\\n{demo.class_names[np.argmax(adv_pred)]}')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 4: UNDERSTANDING ATTACK MECHANICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "visualize_attack_mechanics(demo, num_examples=3, epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bac1b-700e-4003-96ba-52f3b352a0d4",
   "metadata": {},
   "source": [
    "## ðŸ¤” OBSERVATION CHECKPOINT - Part 4\n",
    "\n",
    "**PERTURBATION ANALYSIS:**\n",
    "- The perturbations are amplified **10x** for visibility\n",
    "- Without amplification, would you notice them? ______\n",
    "- Do they look random or structured? ________________\n",
    "\n",
    "**LEARNING CHECK:** Explain FGSM in your own words:\n",
    "___________________________________________________________________\n",
    "___________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c6a3a-2b49-46e3-8472-43ef12e11f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Confidence Analysis\n",
    "def analyze_confidence_changes(demo, num_examples=3, epsilon=EPSILON):\n",
    "    \"\"\"Show how model confidence changes with attacks\"\"\"\n",
    "    print(\"ðŸ“Š Analyzing confidence changes...\")\n",
    "    \n",
    "    indices = np.random.choice(len(demo.x_test), num_examples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(15, 5))\n",
    "    fig.suptitle('Model Confidence: Original vs Adversarial', fontsize=16)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        original_img = demo.x_test[idx:idx+1]\n",
    "        true_label = demo.y_test[idx:idx+1]\n",
    "        true_class = np.argmax(true_label)\n",
    "        \n",
    "        # Generate adversarial example\n",
    "        adv_img = fgsm_attack(demo.model, original_img, true_label, epsilon)\n",
    "        \n",
    "        # Get prediction probabilities\n",
    "        orig_probs = demo.model.predict(original_img, verbose=0)[0]\n",
    "        adv_probs = demo.model.predict(adv_img, verbose=0)[0]\n",
    "        \n",
    "        # Create bar plot\n",
    "        x = np.arange(len(demo.class_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = axes[i].bar(x - width/2, orig_probs, width, label='Original', alpha=0.7, color='blue')\n",
    "        bars2 = axes[i].bar(x + width/2, adv_probs, width, label='Adversarial', alpha=0.7, color='red')\n",
    "        \n",
    "        # Highlight true class\n",
    "        bars1[true_class].set_color('green')\n",
    "        bars2[true_class].set_color('darkred')\n",
    "        \n",
    "        axes[i].set_ylabel('Confidence')\n",
    "        axes[i].set_title(f'True: {demo.class_names[true_class]}')\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(demo.class_names, rotation=45, ha='right')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 5: MODEL CONFIDENCE ANALYSIS\") \n",
    "print(\"=\" * 60)\n",
    "\n",
    "analyze_confidence_changes(demo, num_examples=3, epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff228fcc-c280-4696-b985-bde10d270b39",
   "metadata": {},
   "source": [
    "## ðŸ¤” OBSERVATION CHECKPOINT - Part 5\n",
    "\n",
    "**CONFIDENCE ANALYSIS:**\n",
    "- Does the model become more or less confident in wrong predictions? ________________\n",
    "- Which class appears frequently as an attack target? ________________\n",
    "\n",
    "**What epsilon provides good balance between effectiveness and imperceptibility?**\n",
    "\n",
    "**YOUR ANSWER:** ___________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82533b1-20c2-4e6b-82ed-5f827774d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Adversarial Training (Fixed)\n",
    "def train_robust_model(demo, epochs=EPOCHS_ROBUST, epsilon=EPSILON):\n",
    "    \"\"\"Train a robust model using adversarial training\"\"\"\n",
    "    print(\"ðŸ›¡ï¸ Training robust model with adversarial examples...\")\n",
    "    print(\"This is like training a security system to recognize attack patterns!\")\n",
    "    \n",
    "    # Create new model\n",
    "    demo.robust_model = demo.create_simple_cnn()\n",
    "    \n",
    "    batch_size = 32\n",
    "    num_batches = len(demo.x_train) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            # Get batch\n",
    "            x_batch = demo.x_train[start_idx:end_idx]\n",
    "            y_batch = demo.y_train[start_idx:end_idx]\n",
    "            \n",
    "            # Generate adversarial examples for half the batch using the standard model\n",
    "            adv_indices = np.random.choice(batch_size, batch_size // 2, replace=False)\n",
    "            \n",
    "            x_adv_batch = x_batch.copy()\n",
    "            for idx in adv_indices:\n",
    "                x_single = x_batch[idx:idx+1]\n",
    "                y_single = y_batch[idx:idx+1]\n",
    "                # Key fix: use original model to generate adversarial examples\n",
    "                x_adv_batch[idx] = fgsm_attack(demo.model, x_single, y_single, epsilon)[0]\n",
    "            \n",
    "            # Train on mixed batch (original + adversarial)\n",
    "            x_mixed = np.concatenate([x_batch, x_adv_batch])\n",
    "            y_mixed = np.concatenate([y_batch, y_batch])\n",
    "            \n",
    "            # Train step\n",
    "            loss, acc = demo.robust_model.train_on_batch(x_mixed, y_mixed)\n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_acc = epoch_acc / num_batches\n",
    "        print(f\"  Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "    \n",
    "    # Evaluate robust model properly\n",
    "    test_loss, clean_acc = demo.robust_model.evaluate(demo.x_test, demo.y_test, verbose=0)\n",
    "    print(f\"\\nâœ“ Robust model clean accuracy: {clean_acc:.4f}\")\n",
    "    \n",
    "    # Test adversarial robustness using attacks from the ORIGINAL model\n",
    "    print(\"Testing adversarial robustness against original model attacks...\")\n",
    "    adv_correct = 0\n",
    "    test_samples = 500\n",
    "    \n",
    "    for i in range(test_samples):\n",
    "        original_img = demo.x_test[i:i+1]\n",
    "        true_label = demo.y_test[i:i+1]\n",
    "        true_class = np.argmax(true_label)\n",
    "        \n",
    "        # CRITICAL: Generate adversarial examples using the ORIGINAL vulnerable model\n",
    "        adv_img = fgsm_attack(demo.model, original_img, true_label, epsilon)\n",
    "        \n",
    "        # Test how the ROBUST model handles these attacks\n",
    "        adv_pred = np.argmax(demo.robust_model.predict(adv_img, verbose=0))\n",
    "        \n",
    "        if adv_pred == true_class:\n",
    "            adv_correct += 1\n",
    "    \n",
    "    adv_accuracy = adv_correct / test_samples\n",
    "    print(f\"âœ“ Robust model adversarial accuracy: {adv_accuracy:.4f} ({adv_accuracy*100:.1f}%)\")\n",
    "    print(f\"Defense improvement: Successfully resists {adv_accuracy*100:.1f}% of attacks!\")\n",
    "    \n",
    "    return clean_acc, adv_accuracy\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 6: BUILDING DEFENSES - ADVERSARIAL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "robust_accuracy = train_robust_model(demo, epochs=EPOCHS_ROBUST, epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68764b4-2d24-4d3c-b83b-04f4c0c27060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Fixed Final Comparison\n",
    "def compare_models_final(demo, epsilon=EPSILON, num_samples=200):\n",
    "    \"\"\"Compare standard vs robust model performance - FIXED VERSION\"\"\"\n",
    "    print(\"âš–ï¸ Comparing model performance...\")\n",
    "    \n",
    "    indices = np.random.choice(len(demo.x_test), num_samples, replace=False)\n",
    "    \n",
    "    std_clean_correct = 0\n",
    "    std_adv_correct = 0  \n",
    "    rob_clean_correct = 0\n",
    "    rob_adv_correct = 0\n",
    "    \n",
    "    for idx in indices:\n",
    "        original_img = demo.x_test[idx:idx+1]\n",
    "        true_label = demo.y_test[idx:idx+1]\n",
    "        true_class = np.argmax(true_label)\n",
    "        \n",
    "        # CRITICAL FIX: Generate adversarial examples using ONLY the original standard model\n",
    "        # This represents the real attack scenario - attackers target the original vulnerable system\n",
    "        adv_img = fgsm_attack(demo.model, original_img, true_label, epsilon)\n",
    "        \n",
    "        # Test both models on clean images\n",
    "        std_clean_pred = np.argmax(demo.model.predict(original_img, verbose=0))\n",
    "        rob_clean_pred = np.argmax(demo.robust_model.predict(original_img, verbose=0))\n",
    "        \n",
    "        # Test both models on the SAME adversarial examples (generated from standard model)\n",
    "        std_adv_pred = np.argmax(demo.model.predict(adv_img, verbose=0))\n",
    "        rob_adv_pred = np.argmax(demo.robust_model.predict(adv_img, verbose=0))\n",
    "        \n",
    "        # Count correct predictions\n",
    "        if std_clean_pred == true_class:\n",
    "            std_clean_correct += 1\n",
    "        if std_adv_pred == true_class:\n",
    "            std_adv_correct += 1\n",
    "        if rob_clean_pred == true_class:\n",
    "            rob_clean_correct += 1\n",
    "        if rob_adv_pred == true_class:\n",
    "            rob_adv_correct += 1\n",
    "    \n",
    "    # Calculate percentages\n",
    "    std_clean_acc = std_clean_correct / num_samples * 100\n",
    "    std_adv_acc = std_adv_correct / num_samples * 100\n",
    "    rob_clean_acc = rob_clean_correct / num_samples * 100\n",
    "    rob_adv_acc = rob_adv_correct / num_samples * 100\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nðŸ“Š FINAL COMPARISON RESULTS (Îµ={epsilon})\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Standard Model:\")\n",
    "    print(f\"  Clean Accuracy:       {std_clean_acc:.1f}%\")\n",
    "    print(f\"  Adversarial Accuracy: {std_adv_acc:.1f}%\")\n",
    "    print(f\"\\nRobust Model:\")\n",
    "    print(f\"  Clean Accuracy:       {rob_clean_acc:.1f}%\")\n",
    "    print(f\"  Adversarial Accuracy: {rob_adv_acc:.1f}%\")\n",
    "    print(f\"\\nTrade-offs:\")\n",
    "    print(f\"  Clean Accuracy Lost:  {std_clean_acc - rob_clean_acc:.1f}%\")\n",
    "    print(f\"  Adversarial Gain:     {rob_adv_acc - std_adv_acc:.1f}%\")\n",
    "    \n",
    "    # Enhanced visualization to show the dramatic security trade-off\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Adversarial Training Results: Security vs Performance Trade-off', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Side-by-side accuracy comparison\n",
    "    models = ['Standard\\nModel', 'Robust\\nModel']\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    clean_scores = [std_clean_acc, rob_clean_acc]\n",
    "    adv_scores = [std_adv_acc, rob_adv_acc]\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, clean_scores, width, label='Clean Accuracy', color='lightblue', edgecolor='blue')\n",
    "    bars2 = ax1.bar(x + width/2, adv_scores, width, label='Adversarial Accuracy', color='lightcoral', edgecolor='red')\n",
    "    \n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax1.set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels with better visibility\n",
    "    for i, (clean, adv) in enumerate(zip(clean_scores, adv_scores)):\n",
    "        ax1.text(i - width/2, clean + 2, f'{clean:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        ax1.text(i + width/2, adv + 2, f'{adv:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Dramatic adversarial vulnerability comparison\n",
    "    ax2.bar(models, adv_scores, color=['darkred', 'darkgreen'], alpha=0.8)\n",
    "    ax2.set_ylabel('Adversarial Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title('Adversarial Attack Resistance', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, max(adv_scores) * 1.2)\n",
    "    \n",
    "    # Highlight the dramatic improvement\n",
    "    for i, score in enumerate(adv_scores):\n",
    "        ax2.text(i, score + max(adv_scores)*0.03, f'{score:.1f}%', ha='center', va='bottom', \n",
    "                fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # Add dramatic improvement callout\n",
    "    improvement = rob_adv_acc - std_adv_acc\n",
    "    if improvement > 5:  # Only show improvement if significant\n",
    "        ax2.annotate(f'{improvement:.1f}%\\nImprovement!', \n",
    "                    xy=(1, rob_adv_acc), xytext=(0.5, rob_adv_acc * 0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3', color='gold', lw=3),\n",
    "                    fontsize=14, fontweight='bold', ha='center',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='yellow', alpha=0.9, edgecolor='orange'))\n",
    "    \n",
    "    # 3. Security effectiveness matrix\n",
    "    categories = ['Normal\\nOperation', 'Under\\nAttack']\n",
    "    standard_performance = [std_clean_acc, std_adv_acc]\n",
    "    robust_performance = [rob_clean_acc, rob_adv_acc]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    bars3 = ax3.bar(x - width/2, standard_performance, width, label='Standard Model', \n",
    "                   color=['lightblue', 'darkred'], alpha=0.7)\n",
    "    bars4 = ax3.bar(x + width/2, robust_performance, width, label='Robust Model', \n",
    "                   color=['blue', 'green'], alpha=0.7)\n",
    "    \n",
    "    ax3.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax3.set_title('Security Effectiveness', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(categories)\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (std, rob) in enumerate(zip(standard_performance, robust_performance)):\n",
    "        ax3.text(i - width/2, std + 2, f'{std:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        ax3.text(i + width/2, rob + 2, f'{rob:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Add vulnerability callout for standard model under attack\n",
    "    if std_adv_acc < 20:\n",
    "        ax3.annotate('VULNERABLE!', xy=(1 - width/2, std_adv_acc), \n",
    "                    xytext=(0.3, 40), fontsize=12, fontweight='bold', color='red',\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='red', alpha=0.2))\n",
    "    \n",
    "    # 4. Trade-off analysis\n",
    "    trade_off_data = {\n",
    "        'Security Gain': improvement,\n",
    "        'Accuracy Cost': std_clean_acc - rob_clean_acc\n",
    "    }\n",
    "    \n",
    "    colors = ['green' if improvement > 0 else 'red', 'orange' if std_clean_acc > rob_clean_acc else 'green']\n",
    "    bars = ax4.bar(trade_off_data.keys(), trade_off_data.values(), color=colors, alpha=0.7)\n",
    "    ax4.set_ylabel('Percentage Points', fontsize=12)\n",
    "    ax4.set_title('Trade-off Analysis', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add value labels and interpretation\n",
    "    for bar, value in zip(bars, trade_off_data.values()):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, value + (abs(value)*0.05 if value >= 0 else -abs(value)*0.05), \n",
    "                f'+{value:.1f}%' if value >= 0 else f'{value:.1f}%', \n",
    "                ha='center', va='bottom' if value >= 0 else 'top', \n",
    "                fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Add trade-off interpretation\n",
    "    if improvement > abs(std_clean_acc - rob_clean_acc) * 2:  # If security gain > 2x accuracy cost\n",
    "        ax4.text(0.5, max(trade_off_data.values()) * 0.5, 'EXCELLENT\\nTRADE-OFF!', \n",
    "                ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightgreen', alpha=0.8))\n",
    "    elif improvement > 10:\n",
    "        ax4.text(0.5, max(trade_off_data.values()) * 0.5, 'GOOD\\nTRADE-OFF', \n",
    "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics box\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CYBERSECURITY IMPACT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Standard Model Security Failure: {std_adv_acc:.1f}% accuracy under attack\")\n",
    "    print(f\"Robust Model Security Success: {rob_adv_acc:.1f}% accuracy under attack\")\n",
    "    print(f\"Security Improvement: {improvement:.1f} percentage points\")\n",
    "    print(f\"Accuracy Trade-off: {std_clean_acc - rob_clean_acc:.1f}% clean performance cost\")\n",
    "    print(f\"Net Benefit: {improvement:.1f}% security gain for {abs(std_clean_acc - rob_clean_acc):.1f}% accuracy cost\")\n",
    "    \n",
    "    if improvement > 30:\n",
    "        print(\"\\nCONCLUSION: Adversarial training provides SIGNIFICANT security benefits!\")\n",
    "    elif improvement > 15:\n",
    "        print(\"\\nCONCLUSION: Adversarial training provides meaningful security improvements.\")\n",
    "    elif improvement > 5:\n",
    "        print(\"\\nCONCLUSION: Modest security improvement - consider stronger defenses.\")\n",
    "    else:\n",
    "        print(\"\\nCONCLUSION: Limited improvement - may need different defense strategies.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return std_clean_acc, std_adv_acc, rob_clean_acc, rob_adv_acc\n",
    "    # Final comparison - EXECUTE THE FUNCTION\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 7: FINAL MODEL COMPARISON\") \n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = compare_models_final(demo, epsilon=EPSILON, num_samples=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071d983-d95e-4401-a8ed-8a4c1010a6f4",
   "metadata": {},
   "source": [
    "## ðŸ¤” FINAL OBSERVATION CHECKPOINT - Part 6\n",
    "\n",
    "**DEFENSE RESULTS:**\n",
    "| Model Type | Clean Accuracy | Adversarial Accuracy | Robustness Gain |\n",
    "|------------|----------------|---------------------|----------------|\n",
    "| Standard   | ______%        | ______%             | N/A            |\n",
    "| Robust     | ______%        | ______%             | +______%       |\n",
    "\n",
    "**TRADE-OFF ANALYSIS:**\n",
    "- Clean accuracy lost: ______%  \n",
    "- Adversarial accuracy gained: ______%\n",
    "- Is this trade-off acceptable for cybersecurity? Why?\n",
    "\n",
    "**YOUR ANSWER:** ___________________________________________\n",
    "\n",
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "**Three most important things you learned:**\n",
    "1. ___________________________________________________________\n",
    "2. ___________________________________________________________  \n",
    "3. ___________________________________________________________\n",
    "\n",
    "**For your capstone project, consider:**\n",
    "- How will you test against adversarial attacks?\n",
    "- What defense strategies will you implement?\n",
    "- Is the accuracy/robustness trade-off acceptable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef1f4c-7d5f-4c1a-a7ff-6be03c26291e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
