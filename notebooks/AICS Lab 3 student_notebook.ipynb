{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # PE Malware Classification Analysis - Student Lab Notebook\n",
    "\n",
    "\n",
    "\n",
    " **Course:** AI in Cybersecurity - Class 07\n",
    "\n",
    " **Instructor:** Steve Smith\n",
    "\n",
    " **Duration:** 3-4 hours\n",
    "\n",
    "\n",
    "\n",
    " ## Learning Objectives\n",
    "\n",
    " By the end of this lab, you will be able to:\n",
    "\n",
    " - Distinguish between static and dynamic malware analysis techniques\n",
    "\n",
    " - Load and explore PE file malware datasets for security analysis\n",
    "\n",
    " - Apply advanced feature engineering techniques for malware detection\n",
    "\n",
    " - Train and compare multiple machine learning models\n",
    "\n",
    " - Evaluate model performance using cybersecurity-appropriate metrics\n",
    "\n",
    " - Interpret results and provide actionable security recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Part 1: Understanding Malware Analysis Fundamentals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 1.1: Static vs. Dynamic Analysis Discussion\n",
    "\n",
    "\n",
    "\n",
    " **Before we start coding, let's understand the fundamentals:**\n",
    "\n",
    "\n",
    "\n",
    " #### Discussion Questions:\n",
    "\n",
    " 1. **Static Analysis Advantages:**\n",
    "\n",
    "    - Fast execution\n",
    "\n",
    "    - Safe (no malware execution)\n",
    "\n",
    "    - Can analyze file structure\n",
    "\n",
    "\n",
    "\n",
    " 2. **Dynamic Analysis Advantages:**\n",
    "\n",
    "    - Reveals runtime behavior\n",
    "\n",
    "    - Detects evasion techniques\n",
    "\n",
    "\n",
    "\n",
    " **Write your thoughts below on when you would choose each approach:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Your Analysis:**\n",
    "\n",
    "\n",
    "\n",
    " *Static Analysis is best for:*\n",
    "\n",
    " - [Write your answer here]\n",
    "\n",
    "\n",
    "\n",
    " *Dynamic Analysis is best for:*\n",
    "\n",
    " - [Write your answer here]\n",
    "\n",
    "\n",
    "\n",
    " *Hybrid approaches might include:*\n",
    "\n",
    " - [Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Part 2: Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 2.1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "import warnings\n",
    "\n",
    "# Set up visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîç Malware Classification Analysis\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# TODO: Replace 'dataset_malwares.csv' with the correct path to your dataset\n",
    "df = pd.read_csv('dataset_malwares.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {df.shape[1] - 1}\")\n",
    "print(f\"Samples: {df.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 2.2: Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"üìä First 5 rows of the dataset:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the dataset\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"\\nüîç Missing values: {missing_values.sum()}\")\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 2.3: Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distribution\n",
    "target_dist = df['Malware'].value_counts()\n",
    "print(f\"\\nüéØ Target Distribution:\")\n",
    "print(f\"Malware (1): {target_dist[1]:,} ({target_dist[1]/len(df)*100:.1f}%)\")\n",
    "print(f\"Benign (0): {target_dist[0]:,} ({target_dist[0]/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pie chart for target distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "target_counts = df['Malware'].value_counts()\n",
    "colors = ['#2E8B57', '#DC143C']  # Green for benign, red for malware\n",
    "plt.pie(target_counts.values, labels=['Benign', 'Malware'], autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90)\n",
    "plt.title('Dataset Distribution', fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Analysis Questions:**\n",
    "\n",
    " 1. Is this dataset balanced? **[Write your answer here]**\n",
    "\n",
    " 2. What implications does this have for model training? **[Write your answer here]**\n",
    "\n",
    " 3. What metrics should we prioritize? **[Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Part 3: Comprehensive Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 3.1: PE Header Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key PE header characteristics\n",
    "def create_pe_analysis_plots():\n",
    "    \"\"\"Analyze key PE header characteristics\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Number of Sections Analysis\n",
    "    sns.boxplot(data=df, x='Malware', y='NumberOfSections', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Number of Sections Distribution')\n",
    "    axes[0,0].set_xlabel('Malware (0=Benign, 1=Malware)')\n",
    "    \n",
    "    # Image Size Analysis\n",
    "    sns.boxplot(data=df, x='Malware', y='SizeOfImage', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Image Size Distribution')\n",
    "    axes[0,1].set_xlabel('Malware (0=Benign, 1=Malware)')\n",
    "    axes[0,1].set_yscale('log')\n",
    "    \n",
    "    # Entry Point Analysis\n",
    "    sns.boxplot(data=df, x='Malware', y='AddressOfEntryPoint', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Entry Point Address Distribution')\n",
    "    axes[1,0].set_xlabel('Malware (0=Benign, 1=Malware)')\n",
    "    axes[1,0].set_yscale('log')\n",
    "    \n",
    "    # Entropy Analysis\n",
    "    entropy_data = df[df['SectionMinEntropy'] > 0]\n",
    "    sns.boxplot(data=entropy_data, x='Malware', y='SectionMinEntropy', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Section Minimum Entropy')\n",
    "    axes[1,1].set_xlabel('Malware (0=Benign, 1=Malware)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the analysis\n",
    "create_pe_analysis_plots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Analysis Questions:**\n",
    "\n",
    " 1. Do malware files tend to have more or fewer sections than benign files? **[Your answer]**\n",
    "\n",
    " 2. What patterns do you observe in image sizes between classes? **[Your answer]**\n",
    "\n",
    " 3. Why might entry point addresses differ between malware and benign files? **[Your answer]**\n",
    "\n",
    " 4. What does entropy tell us about the content of file sections? **[Your answer]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 3.2: Suspicious Features Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze suspicious features\n",
    "# Create a bar plot showing the mean values of suspicious features by class\n",
    "suspicious_features = ['SuspiciousImportFunctions', 'SuspiciousNameSection']\n",
    "\n",
    "# Hint: Use groupby('Malware').mean() and plot(kind='bar')\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Discussion:** Why are these features particularly important for malware detection?\n",
    "\n",
    " **[Write your analysis here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 3.3: Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate top feature correlations\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols.remove('Malware')\n",
    "\n",
    "# Get top 15 correlated features\n",
    "target_corr = df[numeric_cols + ['Malware']].corr()['Malware'].abs().sort_values(ascending=False)\n",
    "top_features = target_corr.head(15).index.tolist()\n",
    "\n",
    "print(\"Top 15 features by correlation with malware classification:\")\n",
    "for i, feature in enumerate(top_features[:10]):\n",
    "    print(f\"{i+1:2}. {feature}: {target_corr[feature]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[top_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, cbar_kws={'shrink': 0.8})\n",
    "plt.title('Top 15 Features Correlation Matrix', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a horizontal bar plot of feature importance by correlation\n",
    "# Hint: Use target_corr.head(15).plot(kind='barh')\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Key Questions:**\n",
    "\n",
    " 1. Which features show the strongest correlation with malware classification? **[List top 3]**\n",
    "\n",
    " 2. Are there any highly correlated features that might cause multicollinearity? **[Your analysis]**\n",
    "\n",
    " 3. Which PE characteristics appear most discriminative? **[Your insights]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Part 4: Feature Engineering and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 4.1: Feature Selection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "feature_columns = [col for col in df.columns if col not in ['Name', 'Malware']]\n",
    "X = df[feature_columns].copy()\n",
    "y = df['Malware'].copy()\n",
    "\n",
    "print(f\"Features selected: {len(feature_columns)}\")\n",
    "print(f\"First 10 features: {feature_columns[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values using median imputation\n",
    "X = X.fillna(X.median())\n",
    "print(f\"Missing values after imputation: {X.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low-variance features\n",
    "variance_threshold = VarianceThreshold(threshold=0.01)\n",
    "X_variance_filtered = variance_threshold.fit_transform(X)\n",
    "selected_features = variance_threshold.get_support()\n",
    "feature_names = [feature_columns[i] for i in range(len(feature_columns)) if selected_features[i]]\n",
    "\n",
    "print(f\"Features after variance filtering: {len(feature_names)}\")\n",
    "X = pd.DataFrame(X_variance_filtered, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Discussion:** Why do we remove low-variance features? What impact does this have on model performance?\n",
    "\n",
    " **[Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 4.2: Data Splitting and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data with stratification\n",
    "# Hint: Use train_test_split with test_size=0.2, random_state=42, stratify=y\n",
    "# Your code here:\n",
    "X_train, X_test, y_train, y_test = # Complete this line\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using RobustScaler (better for outliers)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed using RobustScaler\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Key Concepts:**\n",
    "\n",
    " - Why use stratified splitting for imbalanced datasets? **[Your answer]**\n",
    "\n",
    " - Why choose RobustScaler over StandardScaler? **[Your answer]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 4.3: Advanced Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical feature selection\n",
    "selector_stats = SelectKBest(score_func=f_classif, k=30)\n",
    "X_train_selected = selector_stats.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector_stats.transform(X_test_scaled)\n",
    "\n",
    "selected_feature_indices = selector_stats.get_support(indices=True)\n",
    "selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "print(f\"Top 30 features selected: {selected_feature_names[:10]}...\")\n",
    "print(f\"Training data shape after feature selection: {X_train_selected.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Part 5: Machine Learning Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 5.1: Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models for comparison\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "print(\"Models defined:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    y_pred_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'auc_score': auc_score\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Understanding model.fit():**\n",
    "\n",
    " The `model.fit()` method is where the magic happens:\n",
    "\n",
    " - **Training:** Adjusts model parameters to learn patterns in training data\n",
    "\n",
    " - **Learning:** Finds optimal settings that minimize error/loss function\n",
    "\n",
    " - **Preparation:** Once fitted, model can make predictions on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 5.2: Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison\n",
    "print(\"\\nüèÜ Model Performance Ranking:\")\n",
    "for name, results in sorted(model_results.items(), key=lambda x: x[1]['auc_score'], reverse=True):\n",
    "    print(f\"   ‚Ä¢ {name}: AUC = {results['auc_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a bar plot comparing AUC scores\n",
    "# Hint: Extract auc_scores and model_names, then use plt.bar()\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 5.3: Detailed Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['auc_score'])\n",
    "best_model = model_results[best_model_name]\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Best AUC score: {best_model['auc_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "for i, (name, results) in enumerate(model_results.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, results['probabilities'])\n",
    "    plt.plot(fpr, tpr, color=colors[i], linewidth=2, \n",
    "             label=f'{name} (AUC: {results[\"auc_score\"]:.3f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create confusion matrix for best model\n",
    "# Hint: Use sklearn.metrics.confusion_matrix and seaborn.heatmap\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for best model\n",
    "best_predictions = best_model['predictions']\n",
    "report = classification_report(y_test, best_predictions, \n",
    "                             target_names=['Benign', 'Malware'])\n",
    "print(f\"\\nDetailed Classification Report - {best_model_name}:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Key Evaluation Metrics for Cybersecurity:**\n",
    "\n",
    " - **Precision:** Of all positive predictions, how many were correct? (TP / (TP + FP))\n",
    "\n",
    " - **Recall:** Of all actual positives, how many were identified? (TP / (TP + FN))\n",
    "\n",
    " - **F1-Score:** Harmonic mean of precision and recall\n",
    "\n",
    " - **AUC-ROC:** Overall model discriminative ability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Analysis Questions:**\n",
    "\n",
    " 1. Which model performed best and why? **[Your analysis]**\n",
    "\n",
    " 2. What are the precision and recall trade-offs? **[Your analysis]**\n",
    "\n",
    " 3. How would you choose a threshold for production deployment? **[Your analysis]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 5.4: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "if 'Random Forest' in model_results:\n",
    "    rf_model = model_results['Random Forest']['model']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_feature_names,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 15 Most Important Features (Random Forest):\")\n",
    "    print(feature_importance.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a horizontal bar plot of top 15 feature importances\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Part 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 6.1: Advanced Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    print(\"Tuning Random Forest hyperparameters...\")\n",
    "    \n",
    "    # TODO: Define parameter grid for Random Forest\n",
    "    # Include: n_estimators, max_depth, min_samples_split, min_samples_leaf\n",
    "    param_grid = {\n",
    "        # Your parameter grid here\n",
    "    }\n",
    "    \n",
    "    # TODO: Create GridSearchCV object\n",
    "    # Hint: Use cv=3, scoring='roc_auc', n_jobs=-1\n",
    "    grid_search = # Your code here\n",
    "    \n",
    "    # Fit grid search\n",
    "    print(\"Starting grid search (this may take a few minutes)...\")\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final optimized model\n",
    "if 'grid_search' in locals():\n",
    "    final_model = grid_search.best_estimator_\n",
    "    final_predictions = final_model.predict(X_test_selected)\n",
    "    final_probabilities = final_model.predict_proba(X_test_selected)[:, 1]\n",
    "    final_auc = roc_auc_score(y_test, final_probabilities)\n",
    "    \n",
    "    print(f\"Final optimized model AUC: {final_auc:.4f}\")\n",
    "    print(f\"Improvement over baseline: {final_auc - best_model['auc_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Hyperparameter Concepts:**\n",
    "\n",
    " - **Parameters:** Learned during training (weights, biases)\n",
    "\n",
    " - **Hyperparameters:** Set before training (learning rate, tree depth)\n",
    "\n",
    " - **Grid Search:** Systematic search over parameter combinations\n",
    "\n",
    " - **Cross-Validation:** Robust evaluation to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Part 7: Results Analysis and Cybersecurity Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 7.1: Comprehensive Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive results summary\n",
    "print(\"\\nüìã Malware Classification Analysis Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üéØ Dataset Overview:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ Malware samples: {target_dist[1]:,} ({target_dist[1]/len(df)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Benign samples: {target_dist[0]:,} ({target_dist[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Features analyzed: {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\nüèÜ Model Performance Ranking:\")\n",
    "for name, results in sorted(model_results.items(), key=lambda x: x[1]['auc_score'], reverse=True):\n",
    "    print(f\"   ‚Ä¢ {name}: AUC = {results['auc_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the analysis with your findings\n",
    "print(f\"\\nüîç Key Security Findings:\")\n",
    "print(f\"   ‚Ä¢ Best performing model: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Dataset shows class imbalance (malware-heavy)\")\n",
    "# Add more findings based on your analysis\n",
    "\n",
    "print(f\"\\nüí° Production Deployment Recommendations:\")\n",
    "print(f\"   ‚Ä¢ Deploy {best_model_name} for initial implementation\")\n",
    "# Add more recommendations based on your analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 7.2: Security Operations Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a prediction probability distribution plot\n",
    "# Show how well the model separates malware from benign files\n",
    "# Hint: Use histograms of prediction probabilities for each class\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Critical Questions for Production Deployment:**\n",
    "\n",
    "\n",
    "\n",
    " 1. **False Positive Impact:** How would false positives affect business operations?\n",
    "\n",
    "    **[Your analysis]**\n",
    "\n",
    "\n",
    "\n",
    " 2. **False Negative Risk:** What is the cost of missing actual malware?\n",
    "\n",
    "    **[Your analysis]**\n",
    "\n",
    "\n",
    "\n",
    " 3. **Model Drift:** How do we detect when new malware families emerge?\n",
    "\n",
    "    **[Your analysis]**\n",
    "\n",
    "\n",
    "\n",
    " 4. **Explainability:** Can we explain why a file was classified as malware?\n",
    "\n",
    "    **[Your analysis]**\n",
    "\n",
    "\n",
    "\n",
    " 5. **Performance:** What are the real-time classification requirements?\n",
    "\n",
    "    **[Your analysis]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 7.3: Feature Engineering Innovation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create new engineered features\n",
    "# Try creating ratios, combinations, or statistical measures from existing features\n",
    "# Examples:\n",
    "# - Ratio of suspicious imports to total imports\n",
    "# - Entropy variance across sections  \n",
    "# - Section size distribution statistics\n",
    "\n",
    "# Example starter code:\n",
    "# df['sections_per_mb'] = df['NumberOfSections'] / (df['SizeOfImage'] / 1000000)\n",
    "# df['import_density'] = df['DirectoryEntryImportSize'] / df['SizeOfImage']\n",
    "\n",
    "# Your innovative features here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Assignment Submission Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Assignment #04: Feature Engineering & Visualization (15 points)\n",
    "\n",
    " **Due: August 12, 11:59 PM ET**\n",
    "\n",
    "\n",
    "\n",
    " **Before submitting, ensure you have:**\n",
    "\n",
    "\n",
    "\n",
    " - [ ] **Complete Jupyter Notebook** with all sections filled out\n",
    "\n",
    " - [ ] **Quality Visualizations** using seaborn (boxplots, heatmaps, ROC curves)\n",
    "\n",
    " - [ ] **Feature Analysis** identifying and discussing top 10 most important features\n",
    "\n",
    " - [ ] **Model Comparison** with performance metrics and analysis\n",
    "\n",
    " - [ ] **Security Discussion** on production deployment implications\n",
    "\n",
    " - [ ] **Code Comments** explaining your analysis decisions\n",
    "\n",
    " - [ ] **Clear Conclusions** answering all discussion questions\n",
    "\n",
    "\n",
    "\n",
    " **Grading Breakdown:**\n",
    "\n",
    " - Code Quality (4 pts): Clean, commented, functional code\n",
    "\n",
    " - Visualizations (4 pts): Clear, insightful, well-labeled plots\n",
    "\n",
    " - Feature Analysis (3 pts): Identification and interpretation of key features\n",
    "\n",
    " - Model Evaluation (2 pts): Appropriate metrics and comparison\n",
    "\n",
    " - Security Insights (2 pts): Real-world implications discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Final Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Complete this reflection on your learning:**\n",
    "\n",
    "\n",
    "\n",
    " 1. **What was the most surprising finding in your analysis?**\n",
    "\n",
    "    [Your answer here]\n",
    "\n",
    "\n",
    "\n",
    " 2. **Which PE file features were most important for malware detection and why?**\n",
    "\n",
    "    [Your answer here]\n",
    "\n",
    "\n",
    "\n",
    " 3. **How would you improve this analysis for a production security system?**\n",
    "\n",
    "    [Your answer here]\n",
    "\n",
    "\n",
    "\n",
    " 4. **What additional data sources might enhance malware detection?**\n",
    "\n",
    "    [Your answer here]\n",
    "\n",
    "\n",
    "\n",
    " 5. **How does this lab connect to real-world cybersecurity operations?**\n",
    "\n",
    "    [Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Congratulations! üéâ\n",
    "\n",
    "\n",
    "\n",
    " You've completed a comprehensive malware classification analysis using machine learning. The skills you've developed here are directly applicable to:\n",
    "\n",
    "\n",
    "\n",
    " - **Security Operations Centers (SOCs):** Automated threat detection\n",
    "\n",
    " - **Threat Intelligence:** Malware family classification\n",
    "\n",
    " - **Incident Response:** Rapid malware identification\n",
    "\n",
    " - **Security Product Development:** AV/EDR solutions\n",
    "\n",
    "\n",
    "\n",
    " **Next Steps:**\n",
    "\n",
    " - Review your analysis and ensure all sections are complete\n",
    "\n",
    " - Double-check your visualizations and interpretations\n",
    "\n",
    " - Submit your completed notebook for grading\n",
    "\n",
    " - Continue exploring advanced topics in adversarial AI and security automation\n",
    "\n",
    "\n",
    "\n",
    " **Great work! You're now ready to apply AI to cybersecurity challenges! üõ°Ô∏èü§ñ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
